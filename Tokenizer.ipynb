{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqFF6TDuQ+huD4hstQ+6MO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manjunath-hanmantgad/Natural-language-Processing/blob/development/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrained Sentence Tokenizer Models\n",
        "\n",
        "\n",
        "We can use sent_tokenize, which is already trained, or load a pretrained tokenization model on German text into a PunktSentenceTokenizer instance and perform the same operation. "
      ],
      "metadata": {
        "id": "yNQm6KTa8FCU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7GtlKMq77zl",
        "outputId": "cd5f6f5c-e02c-41db-8110-e56dc8c750d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157171\n",
            " \n",
            "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n",
            "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
            "True\n",
            "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .'\n",
            " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .'\n",
            " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .'\n",
            " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .'\n",
            " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Package europarl_raw is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# load text\n",
        "# apply tokenizer\n",
        "# check if tokenizer used is returning correct result\n",
        "# display sample text after tokenization\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('europarl_raw')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import europarl_raw\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "print(german_text[0:100])\n",
        "\n",
        "# tokenize the text \n",
        "\n",
        "# first using default sentence tokenizer \n",
        "default_st = nltk.sent_tokenize\n",
        "german_sentences_def = default_st(text=german_text, language=\"german\")\n",
        "\n",
        "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "\n",
        "german_sentences = german_tokenizer.tokenize(german_text)\n",
        "\n",
        "# verify the type of german_tokenizer\n",
        "# should be PunktSentenceTokenizer\n",
        "print(type(german_tokenizer))\n",
        "\n",
        "# check if results of both tokenizers match\n",
        "# should be True\n",
        "print(german_sentences_def == german_sentences)\n",
        "\n",
        "'''\n",
        "we see that indeed the german_tokenizer is an instance of PunktSentenceTokenizer, which specializes in dealing with the German language. \n",
        "'''\n",
        "\n",
        "# print first 5 sentences of the corpus\n",
        "print(np.array(german_sentences[:5]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text\n",
        "# apply tokenizer\n",
        "# check if tokenizer used is returning correct result\n",
        "# display sample text after tokenization\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('europarl_raw')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import europarl_raw\n",
        "\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "print(german_text[0:100])\n",
        "\n",
        "# tokenize the text \n",
        "\n",
        "# first using default sentence tokenizer \n",
        "default_st = nltk.sent_tokenize\n",
        "german_sentences_def = default_st(text=german_text, language=\"german\")\n",
        "\n",
        "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "\n",
        "german_sentences = german_tokenizer.tokenize(german_text)\n",
        "\n",
        "# verify the type of german_tokenizer\n",
        "# should be PunktSentenceTokenizer\n",
        "print(type(german_tokenizer))\n",
        "\n",
        "# check if results of both tokenizers match\n",
        "# should be True\n",
        "print(german_sentences_def == german_sentences)\n",
        "\n",
        "'''\n",
        "we see that indeed the german_tokenizer is an instance of PunktSentenceTokenizer, which specializes in dealing with the German language. \n",
        "'''\n",
        "\n",
        "# print first 5 sentences of the corpus\n",
        "print(np.array(german_sentences[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oflC8ik-CpmM",
        "outputId": "411149b0-4b42-4ee3-d937-cdda9dafd6da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157171\n",
            " \n",
            "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n",
            "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
            "True\n",
            "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .'\n",
            " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .'\n",
            " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .'\n",
            " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .'\n",
            " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Package europarl_raw is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RegexpTokenizer\n",
        "\n",
        "using an instance of the RegexpTokenizer class to tokenize text into sentences, where we will use specific regular expression-based patterns to segment sentences."
      ],
      "metadata": {
        "id": "XJigN1T4DIs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(\n",
        "            pattern=SENTENCE_TOKENS_PATTERN,\n",
        "            gaps=True)\n",
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \"\n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "\n",
        "# applying regexp tokenizer\n",
        "\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "\n",
        "print(np.array(sample_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx5e1SKLCuPL",
        "outputId": "4770fb7e-ff81-4dd4-cff0-9d7990cf239f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Tokenizer/zation"
      ],
      "metadata": {
        "id": "CbRNh_wVDr8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split words to build sentences\n",
        "# stemming and lemmatization work on individual words\n",
        "\n",
        "'''\n",
        "word_tokenize\n",
        "\n",
        "TreebankWordTokenizer\n",
        "\n",
        "TokTokTokenizer\n",
        "\n",
        "RegexpTokenizer\n",
        "\n",
        "Inherited tokenizers from RegexpTokenizer\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4eFct_REDrPl",
        "outputId": "5acf215f-ae8c-4e88-be35-8fa1a7096334"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nword_tokenize\\n\\nTreebankWordTokenizer\\n\\nTokTokTokenizer\\n\\nRegexpTokenizer\\n\\nInherited tokenizers from RegexpTokenizer\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default Word Tokenizer\n",
        "\n",
        "'''\n",
        "The nltk.word_tokenize(...) function is the default and recommended word tokenizer, as specified by NLTK. This tokenizer is an instance or object of the TreebankWordTokenizer class in its internal implementation and acts as a wrapper to that core class\n",
        "'''\n",
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sample_text)\n",
        "np.array(words)\n",
        "\n",
        "# TreebankWordTokenizer\n",
        "\n",
        "# TreebankWordTokenizer is based on the Penn Treebank and uses various regular expressions to tokenize the text.\n",
        "\n",
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "words = treebank_wt.tokenize(sample_text)\n",
        "np.array(words)\n",
        "\n",
        "# the output is similar to word_tokenize(), since they use the same tokenizing mechanism.\n",
        "\n",
        "# TokTokTokenizer\n",
        "\n",
        "'''\n",
        "the tok-tok tokenizer is a general tokenizer, where it assumes that the input has one sentence per lin\n",
        "\n",
        "'''\n",
        "\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "words = tokenizer.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZq2ccocJB1Q",
        "outputId": "ac0e2304-4112-458e-b3e0-291c412db1e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizers with nltk + Spacy"
      ],
      "metadata": {
        "id": "gDWoZUe9UB42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens\n",
        "sents = tokenize_text(sample_text)\n",
        "np.array(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-OmgMEVUFMR",
        "outputId": "932f9239-8fcf-4657-c0e4-571e8469b665"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e5ff2c91e57e>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.array(sents)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenization\n",
        "words = [word for sentence in sents for word in sentence]\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZUlgx_SUvmF",
        "outputId": "c6ade24f-07d3-4175-e12b-caab1dc9423f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # using spacy \n",
        "# import spacy\n",
        "# #nlp = spacy.load('en_core', parse = True, tag=True, entity=True)\n",
        "# #nlp = spacy.load('en_core')\n",
        "# text_spacy = nlp(sample_text)\n",
        "# sents = np.array(list(text_spacy.sents))\n",
        "# sents"
      ],
      "metadata": {
        "id": "pveeFhlgU7Rg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Accented Characters"
      ],
      "metadata": {
        "id": "sP1q-6oKVo84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZOofTPcvVT9a",
        "outputId": "d6736c2c-8774-411b-a7c2-c0583ea48c5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Special Characters"
      ],
      "metadata": {
        "id": "eu7NNKafV87T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "remove_special_characters(\"Well this was fun! What do you think? 123#@!\",\n",
        "                          remove_digits=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zCAX-321Vr74",
        "outputId": "78ff207e-5e99-4065-9f62-630231d69e50"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case conversions\n",
        "# lower - upper\n",
        "# upper - lower\n",
        "\n",
        "# lowercase\n",
        "text = 'The quick brown fox jumped over The Big Dog'\n",
        "text.lower()\n",
        "'the quick brown fox jumped over the big dog'\n",
        "# uppercase\n",
        "text.upper()\n",
        "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'\n",
        "# title case\n",
        "text.title()\n",
        "'The Quick Brown Fox Jumped Over The Big Dog'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yrSggfW-WEMH",
        "outputId": "ce07a768-f61e-4ca8-a5cd-afcb9756885f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Quick Brown Fox Jumped Over The Big Dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "JFKb6y_PWU6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtaining the base form of the word.\n",
        "\n",
        "# porter stemmer\n",
        "# lancaster stemmer"
      ],
      "metadata": {
        "id": "cx4XxwlNWQmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "KvG7FFZ6XRag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNet is lemmatizer of nltk"
      ],
      "metadata": {
        "id": "GFX4N2KvXS88"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "NYeYrnvqX72n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "nltk.download('stopwords')\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuFuJ16BX7lW",
        "outputId": "d686059d-b26f-40b9-81de-706ddd0070f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")\n",
        "', , stopwords , computer'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NfvN3GhTXk11",
        "outputId": "77f1e8f9-6c41-489c-ee85-869f36507f2f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', , stopwords , computer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Normalizer"
      ],
      "metadata": {
        "id": "3dGuBUZ-YVyR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Vvu-VQ0YVbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAdbXHRSYLPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}